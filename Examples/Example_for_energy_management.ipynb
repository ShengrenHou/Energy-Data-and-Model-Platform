{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NOTEBOOK_HEADER-->\n",
    "This notebook contain AI based energy transition examples from [AI energy transition group](https://github.com/ShengrenHou/Energy-Data-and-Model-Platform);\n",
    "content is available [on Github](https://github.com/ShengrenHou/Energy-Data-and-Model-Platform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JFOS4zDJ6Gl"
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "1.1 60 Minutes to Deep reinforcement learning based Energy Management: An mock up energy system example\n",
    "\n",
    "<p><a href=\"https://colab.research.google.com/github/ShengrenHou/Energy-Data-and-Model-Platform/blob/main/Examples/Example_for_forecast_HSR_revise.ipynb\"> <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open in Google Colaboratory\"></a><p><a href=\"https://github.com/ShengrenHou/Energy-Data-and-Model-Platform/blob/main/Examples/Example_for_forecast_HSR_revise.ipynb\"> <img align=\"left\" src=\"https://img.shields.io/badge/Github-Download-blue.svg\" alt=\"Download\" title=\"Download Notebook\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYtO0uLGxtSa"
   },
   "source": [
    "### Energy Management "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy management is the process of monitoring, controlling, and conserving energy in a building or system. It involves the use of technology and systems to optimize energy consumption, reduce waste and lower costs. Effective energy management can result in significant savings on energy bills, reduce the environmental impact of energy consumption and improve overall energy efficiency. Energy management can be applied to a wide range of contexts, including residential, commercial, and industrial settings.\n",
    "\n",
    "References:\n",
    "-[Performance Comparison of Deep RL Algorithms for Energy Systems Optimal Scheduling](https://ieeexplore.ieee.org/document/9960642)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Algorithms\n",
    "\n",
    "Reinforcement Learning (RL) is a subfield of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or punishments based on the actions it takes, and the goal of the agent is to maximize its cumulative reward over time.\n",
    "\n",
    "In RL, the agent learns through trial and error, adjusting its behavior based on the feedback it receives from the environment. The agent uses a policy to map observations of the environment to actions, and it can either learn a new policy through exploration or improve an existing policy through exploitation. RL has been successfully applied to a variety of tasks, including game playing, robotics, and autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYERHpg0yTT2"
   },
   "source": [
    "## Package preparation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjS46YROh613"
   },
   "source": [
    "Install [PyTorch](https://pytorch.org/). \n",
    "The implemented DDPG algorithm is based on PyTorch package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGx1LitkWxRi",
    "outputId": "40d12033-6d8d-445b-9886-e72971431aa8"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Du-4E4TvyR18"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from torch.nn.modules import loss\n",
    "from random_generator_battery import ESSEnv\n",
    "import pandas as pd\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc4z7MDTx91N"
   },
   "source": [
    "## Data and Environment Prepartion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L3Wx7jeyEVr"
   },
   "source": [
    "### Data download and read "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yejJpc6dpcED"
   },
   "source": [
    "Download data from our [github](https://github.com/ShengrenHou/Energy-Data-and-Model-Platform) link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjPXgzbugFYA",
    "outputId": "a8fd133d-f8c2-4c3a-c3f0-6a3e16ab1cbb"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ShengrenHou/Energy-Data-and-Model-Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UPVsIoBpt-i"
   },
   "source": [
    "Read csv data as pandas dataframe and use the first column as the index (time index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "ra-R4rM6ioHR",
    "outputId": "3794f78b-b57f-4d4f-8c11-12135b1084a7"
   },
   "outputs": [],
   "source": [
    "pv_data=pd.read_csv('./Energy-Data-and-Model-Platform/Data/PV.csv')\n",
    "h4_data=pd.read_csv('./Energy-Data-and-Model-Platform/Data/H4.csv')\n",
    "price_data=pd.read_csv('./Energy-Data-and-Model-Platform/Data/Prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3g-7-rtnp7br"
   },
   "source": [
    "### Define Environment\n",
    "\n",
    "An environment for energy management typically includes a model of the energy system being managed, which may include various components such as generators, storage systems, loads, and renewable energy sources. The environment may also include a set of constraints and objectives that the energy management algorithm needs to satisfy, such as power balance constraints, economic cost minimization, or environmental impact minimization.\n",
    "\n",
    "In a reinforcement learning context, the environment provides feedback to the learning agent in the form of rewards or penalties based on the actions it takes. The agent's goal is to learn a policy that maximizes the cumulative reward over time, while satisfying the constraints and objectives of the energy management problem.\n",
    "\n",
    "The specific details of the environment will depend on the particular energy management problem being addressed, such as the type of energy system, the constraints and objectives, and the available data and sensors.\n",
    "To define an environment for a energy management problem, you can start by considering the following components:\n",
    "State Space: \n",
    "\n",
    "Action Space: \n",
    "\n",
    "Reward Function:\n",
    " \n",
    "Dynamics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for battery and DGs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbpVU7JDmq01",
    "outputId": "58802953-0fd2-4d40-879d-fa100de78b34"
   },
   "outputs": [],
   "source": [
    "battery_parameters={\n",
    "'capacity':500,# kw\n",
    "'max_charge':100, # kw\n",
    "'max_discharge':100, #kw\n",
    "'efficiency':0.9,\n",
    "'degradation':0, #euro/kw\n",
    "'max_soc':0.8,\n",
    "'min_soc':0.2,\n",
    "'initial_capacity':0.2}\n",
    "\n",
    "dg_parameters={\n",
    "'gen_1':{'a':0.0034\n",
    ",'b': 3 \n",
    ",'c':30\n",
    ",'d': 0.03,'e':4.2,'f': 0.031,'power_output_max':150,'power_output_min':10,'heat_output_max':None,'heat_output_min':None,\\\n",
    "'ramping_up':100,'ramping_down':100,'min_up':2,'min_down':1},\n",
    "\n",
    "'gen_2':{'a':0.001\n",
    ",'b': 10\n",
    ",'c': 40\n",
    ",'d': 0.03,'e':4.2,'f': 0.031,'power_output_max':375,'power_output_min':50,'heat_output_max':None,'heat_output_min':None,\\\n",
    "    'ramping_up':100,'ramping_down':100,'min_up':2,'min_down':1},\n",
    "\n",
    "'gen_3':{'a':0.001\n",
    ",'b': 15\n",
    ",'c': 70\n",
    ",'d': 0.03,'e':4.2,'f': 0.031,'power_output_max':500,'power_output_min':100,'heat_output_max':None,'heat_output_min':None,\\\n",
    "    'ramping_up':200,'ramping_down':200,'min_up':2,'min_down':1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant:\n",
    "\tMONTHS_LEN = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\tMAX_STEP_HOURS = 24 * 30\n",
    "class DataManager():\n",
    "    def __init__(self) -> None:\n",
    "        self.PV_Generation=[]\n",
    "        self.Prices=[]\n",
    "        self.Electricity_Consumption=[]\n",
    "    def add_pv_element(self,element):self.PV_Generation.append(element)\n",
    "    def add_price_element(self,element):self.Prices.append(element)\n",
    "    def add_electricity_element(self,element):self.Electricity_Consumption.append(element)\n",
    "\n",
    "    # get current time data based on given month day, and day_time\n",
    "    def get_pv_data(self,month,day,day_time):return self.PV_Generation[(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24+day_time]\n",
    "    def get_price_data(self,month,day,day_time):return self.Prices[(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24+day_time]\n",
    "    def get_electricity_cons_data(self,month,day,day_time):return self.Electricity_Consumption[(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24+day_time]\n",
    "    # get series data for one episode\n",
    "    def get_series_pv_data(self,month,day): return self.PV_Generation[(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24:(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24+24]\n",
    "    def get_series_price_data(self,month,day):return self.Prices[(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24:(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24+24]\n",
    "    def get_series_electricity_cons_data(self,month,day):return self.Electricity_Consumption[(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24:(sum(Constant.MONTHS_LEN[:month-1])+day-1)*24+24]\n",
    "\n",
    "class DG():\n",
    "    '''simulate a simple diesel generator here'''\n",
    "    def __init__(self,parameters):\n",
    "        self.name=parameters.keys()\n",
    "        self.a_factor=parameters['a']\n",
    "        self.b_factor=parameters['b']\n",
    "        self.c_factor=parameters['c']\n",
    "        self.power_output_max=parameters['power_output_max']\n",
    "        self.power_output_min=parameters['power_output_min']\n",
    "        self.ramping_up=parameters['ramping_up']\n",
    "        self.ramping_down=parameters['ramping_down']\n",
    "        self.last_step_output=None \n",
    "    def step(self,action_gen):\n",
    "        output_change=action_gen*self.ramping_up# constrain the output_change with ramping up boundary\n",
    "        output=self.current_output+output_change\n",
    "        if output>0:\n",
    "            output=max(self.power_output_min,min(self.power_output_max,output))# meet the constrain \n",
    "        else:\n",
    "            output=0\n",
    "        self.current_output=output\n",
    "    def _get_cost(self,output):\n",
    "        if output<=0:\n",
    "            cost=0\n",
    "        else:\n",
    "            cost=(self.a_factor*pow(output,2)+self.b_factor*output+self.c_factor)\n",
    "        return cost \n",
    "    def reset(self):\n",
    "        self.current_output=0\n",
    "\n",
    "class Battery():\n",
    "    '''simulate a simple battery here'''\n",
    "    def __init__(self,parameters):\n",
    "        self.capacity=parameters['capacity']\n",
    "        self.max_soc=parameters['max_soc']\n",
    "        self.initial_capacity=parameters['initial_capacity']\n",
    "        self.min_soc=parameters['min_soc']# 0.2\n",
    "        self.degradation=parameters['degradation']# degradation cost 1.2\n",
    "        self.max_charge=parameters['max_charge']# nax charge ability\n",
    "        self.max_discharge=parameters['max_discharge']\n",
    "        self.efficiency=parameters['efficiency']\n",
    "    def step(self,action_battery):\n",
    "        energy=action_battery*self.max_charge\n",
    "        updated_capacity=max(self.min_soc,min(self.max_soc,(self.current_capacity*self.capacity+energy)/self.capacity))\n",
    "        self.energy_change=(updated_capacity-self.current_capacity)*self.capacity# if charge, positive, if discharge, negative\n",
    "        self.current_capacity=updated_capacity# update capacity to current codition\n",
    "    def _get_cost(self,energy):# calculate the cost depends on the energy change\n",
    "        cost=energy**2*self.degradation\n",
    "        return cost  \n",
    "    def SOC(self):\n",
    "        return self.current_capacity\n",
    "    def reset(self):\n",
    "        self.current_capacity=np.random.uniform(0.2,0.8)\n",
    "class Grid():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.on=True\n",
    "        if self.on:\n",
    "            self.exchange_ability=100\n",
    "        else:\n",
    "            self.exchange_ability=0\n",
    "    def _get_cost(self,current_price,energy_exchange):\n",
    "        return current_price*energy_exchange\n",
    "    def retrive_past_price(self):\n",
    "        result=[]\n",
    "        if self.day<1:\n",
    "            past_price=self.past_price#\n",
    "        else:\n",
    "            past_price=self.price[24*(self.day-1):24*self.day]\n",
    "            # print(past_price)\n",
    "        for item in past_price[(self.time-24)::]:\n",
    "            result.append(item)\n",
    "        for item in self.price[24*self.day:(24*self.day+self.time)]:\n",
    "            result.append(item)\n",
    "        return result \n",
    "class ESSEnv(gym.Env):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(ESSEnv,self).__init__()\n",
    "        #parameters \n",
    "        self.data_manager=DataManager()\n",
    "        self._load_year_data()\n",
    "        self.episode_length=kwargs.get('episode_length',24)\n",
    "        self.month=None\n",
    "        self.day=None\n",
    "        self.TRAIN=True\n",
    "        self.current_time=None\n",
    "        self.battery_parameters=kwargs.get('battery_parameters',battery_parameters)\n",
    "        self.dg_parameters=kwargs.get('dg_parameters',dg_parameters)\n",
    "        self.penalty_coefficient=50#control soft penalty constrain \n",
    "        self.sell_coefficient=0.5# control sell benefits\n",
    "\n",
    "        self.grid=Grid()\n",
    "        self.battery=Battery(self.battery_parameters)\n",
    "        self.dg1=DG(self.dg_parameters['gen_1'])\n",
    "        self.dg2=DG(self.dg_parameters['gen_2'])\n",
    "        self.dg3=DG(self.dg_parameters['gen_3'])\n",
    "\n",
    "        self.action_space=spaces.Box(low=-1,high=1,shape=(4,),dtype=np.float32)\n",
    "\n",
    "        self.state_space=spaces.Box(low=0,high=1,shape=(7,),dtype=np.float32)\n",
    "\n",
    "    @property\n",
    "    def netload(self):\n",
    "\n",
    "        return self.demand-self.grid.wp_gen-self.grid.pv_gen\n",
    "        \n",
    "    def reset(self,):\n",
    "        self.month=np.random.randint(1,13)# here we choose 12 month\n",
    "        if self.TRAIN:\n",
    "            self.day=np.random.randint(1,20)\n",
    "        else:\n",
    "            self.day=np.random.randint(20,Constant.MONTHS_LEN[self.month]-1)\n",
    "        self.current_time=0\n",
    "        self.battery.reset()\n",
    "        self.dg1.reset()\n",
    "        self.dg2.reset()\n",
    "        self.dg3.reset()\n",
    "        return self._build_state()\n",
    "    def _build_state(self):\n",
    "        soc=self.battery.SOC()\n",
    "        dg1_output=self.dg1.current_output\n",
    "        dg2_output=self.dg2.current_output\n",
    "        dg3_output=self.dg3.current_output\n",
    "        time_step=self.current_time\n",
    "        electricity_demand=self.data_manager.get_electricity_cons_data(self.month,self.day,self.current_time)\n",
    "        pv_generation=self.data_manager.get_pv_data(self.month,self.day,self.current_time)\n",
    "        price=self.data_manager.get_price_data(self.month,self.day,self.current_time)\n",
    "        net_load=electricity_demand-pv_generation\n",
    "        obs=np.concatenate((np.float32(time_step),np.float32(price),np.float32(soc),np.float32(net_load),np.float32(dg1_output),np.float32(dg2_output),np.float32(dg3_output)),axis=None)\n",
    "        return obs\n",
    "\n",
    "    def step(self,action):# state transition here current_obs--take_action--get reward-- get_finish--next_obs\n",
    "        ## here we want to put take action into each components\n",
    "        current_obs=self._build_state()\n",
    "        self.battery.step(action[0])# here execute the state-transition part, battery.current_capacity also changed\n",
    "        self.dg1.step(action[1])\n",
    "        self.dg2.step(action[2])\n",
    "        self.dg3.step(action[3])\n",
    "        current_output=np.array((self.dg1.current_output,self.dg2.current_output,self.dg3.current_output,-self.battery.energy_change))#truely corresonding to the result\n",
    "        self.current_output=current_output\n",
    "        actual_production=sum(current_output)        \n",
    "        netload=current_obs[3]\n",
    "        price=current_obs[1]\n",
    "\n",
    "        unbalance=actual_production-netload\n",
    "\n",
    "        reward=0\n",
    "        excess_penalty=0\n",
    "        deficient_penalty=0\n",
    "        sell_benefit=0\n",
    "        buy_cost=0\n",
    "        self.excess=0\n",
    "        self.shedding=0\n",
    "        if unbalance>=0:# it is now in excess condition\n",
    "            if unbalance<=self.grid.exchange_ability:\n",
    "                sell_benefit=self.grid._get_cost(price,unbalance)*self.sell_coefficient #sell money to grid is little [0.029,0.1]\n",
    "            else:\n",
    "                sell_benefit=self.grid._get_cost(price,self.grid.exchange_ability)*self.sell_coefficient\n",
    "                #real unbalance that even grid could not meet \n",
    "                self.excess=unbalance-self.grid.exchange_ability\n",
    "                excess_penalty=self.excess*self.penalty_coefficient\n",
    "        else:# unbalance <0, its load shedding model, in this case, deficient penalty is used \n",
    "            if abs(unbalance)<=self.grid.exchange_ability:\n",
    "                buy_cost=self.grid._get_cost(price,abs(unbalance))\n",
    "            else:\n",
    "                buy_cost=self.grid._get_cost(price,self.grid.exchange_ability)\n",
    "                self.shedding=abs(unbalance)-self.grid.exchange_ability\n",
    "                deficient_penalty=self.shedding*self.penalty_coefficient\n",
    "        battery_cost=self.battery._get_cost(self.battery.energy_change)# we set it as 0 this time \n",
    "        dg1_cost=self.dg1._get_cost(self.dg1.current_output)\n",
    "        dg2_cost=self.dg2._get_cost(self.dg2.current_output)\n",
    "        dg3_cost=self.dg3._get_cost(self.dg3.current_output)\n",
    "\n",
    "        reward-=(battery_cost+dg1_cost+dg2_cost+dg3_cost+excess_penalty+\n",
    "        deficient_penalty-sell_benefit+buy_cost)/1e3\n",
    "        self.operation_cost=battery_cost+dg1_cost+dg2_cost+dg3_cost+buy_cost-sell_benefit+excess_penalty+deficient_penalty\n",
    "        self.unbalance=unbalance\n",
    "        self.real_unbalance=self.shedding+self.excess\n",
    "        final_step_outputs=[self.dg1.current_output,self.dg2.current_output,self.dg3.current_output,self.battery.current_capacity]\n",
    "        self.current_time+=1\n",
    "        finish=(self.current_time==self.episode_length)\n",
    "        if finish:\n",
    "            self.final_step_outputs=final_step_outputs\n",
    "            self.current_time=0\n",
    "            # self.day+=1\n",
    "            # if self.day>Constant.MONTHS_LEN[self.month-1]:\n",
    "            #     self.day=1\n",
    "            #     self.month+=1\n",
    "            # if self.month>12:\n",
    "            #     self.month=1\n",
    "            #     self.day=1\n",
    "            next_obs=self.reset()\n",
    "            \n",
    "        else:\n",
    "            next_obs=self._build_state()\n",
    "        return current_obs,next_obs,float(reward),finish\n",
    "    def render(self, current_obs, next_obs, reward, finish):\n",
    "        print('day={},hour={:2d}, state={}, next_state={}, reward={:.4f}, terminal={}\\n'.format(self.day,self.current_time, current_obs, next_obs, reward, finish))\n",
    "    def _load_year_data(self):\n",
    "        pv_df=pv_data\n",
    "        #hourly price data for a year \n",
    "        price_df=price_data        # mins electricity consumption data for a year \n",
    "        electricity_df=h4_data\n",
    "        pv_data=pv_df['P_PV_'].apply(lambda x: x.replace(',','.')).to_numpy(dtype=float)\n",
    "        price=price_df['Price'].apply(lambda x:x.replace(',','.')).to_numpy(dtype=float)\n",
    "        electricity=electricity_df['Power'].apply(lambda x:x.replace(',','.')).to_numpy(dtype=float)\n",
    "        # netload=electricity-pv_data\n",
    "        '''we carefully redesign the magnitude for price and amount of generation as well as demand'''\n",
    "        for element in pv_data:\n",
    "            self.data_manager.add_pv_element(element*200)\n",
    "        for element in price:\n",
    "            element/=10\n",
    "            if element<=0.5:\n",
    "                element=0.5\n",
    "            self.data_manager.add_price_element(element)\n",
    "        for i in range(0,electricity.shape[0],60):\n",
    "            element=electricity[i:i+60]\n",
    "            self.data_manager.add_electricity_element(sum(element)*300)\n",
    "if __name__ == '__main__':\n",
    "    env=ESSEnv()\n",
    "    env.TRAIN=False\n",
    "    rewards=[]\n",
    "\n",
    "    current_obs=env.reset()\n",
    "    tem_action=[0.1,0.1,0.1,0.1]\n",
    "    for _ in range (144):\n",
    "        print(f'current month is {env.month}, current day is {env.day}, current time is {env.current_time}')\n",
    "        current_obs,next_obs,reward,finish=env.step(tem_action)\n",
    "        env.render(current_obs,next_obs,reward,finish)\n",
    "        current_obs=next_obs\n",
    "        rewards.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5sB8p0Aym4O"
   },
   "source": [
    "Rearrange data, so we have daily indices with hourly values in the columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjJN6o6YyzfM"
   },
   "outputs": [],
   "source": [
    "grouped_prices = data.loc[:, ['Price']].groupby(data.index.hour)\n",
    "daily_prices = pd.DataFrame(index=data.loc[data.index.hour==0].index, columns=range(24), data={h: grouped_prices.get_group(h).values.flatten() for h in range(24)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWQJy8oEy0Zn"
   },
   "outputs": [],
   "source": [
    "grouped_load = data.loc[:, ['Load']].groupby(data.index.hour)\n",
    "daily_load = pd.DataFrame(index=data.loc[data.index.hour==0].index, columns=range(24), data={h: grouped_load.get_group(h).values.flatten() for h in range(24)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gzxf6aYy88L"
   },
   "source": [
    "Make a feature dataframe with lagged loads and prices. We include the data of the last three days, and the same day last week (d-1, d-2, d-3, d-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJyREM6szVdn"
   },
   "outputs": [],
   "source": [
    "lags = [1, 2, 3, 7]\n",
    "\n",
    "X = pd.concat([daily_prices.shift(l) for l in lags] + [daily_load.shift(l) for l in lags], axis=1).astype(float)\n",
    "X.columns = [f'P_{h}_{l}' for l in lags for h in range(24)] + [f'L_{h}_{l}' for l in lags for h in range(24)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVEQMbh_zN01"
   },
   "source": [
    "Now add the day of the week as a feature. We make a difference for the LASSO (linear model) and the Neural Network (non-linear model). LASSO needs one-hot encoding for these features, while the neural network can deal with a single weekday column having integers representing days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TgnIi7nzbhy"
   },
   "outputs": [],
   "source": [
    "X_lasso = X.copy()\n",
    "for d in range(7):\n",
    "    X_lasso.loc[X_lasso.index.weekday==d, f'D_{d}']=1\n",
    "    X_lasso.loc[X_lasso.index.weekday!=d, f'D_{d}']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfW2vuzZz6XX"
   },
   "outputs": [],
   "source": [
    "X_mlp = X.copy()\n",
    "X_mlp.loc[:, 'DOW'] = X_mlp.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrt8g3vVz8dn"
   },
   "outputs": [],
   "source": [
    "X_lasso.dropna(inplace=True)\n",
    "X_mlp.dropna(inplace=True)\n",
    "\n",
    "y = daily_prices.loc[(daily_prices.index.isin(X_lasso.index)) & (daily_prices.index.isin(X_mlp.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGp3HMViz_9W"
   },
   "source": [
    "Split the data in train-test sets. We use 2019 as test set and the rest as training set. For the lasso we use a different model per hour, and use a single train-test split where we decide the lasso-coefficient on the training data only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RL algorithms DDPG and auxiliary functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DDPG algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, mid_dim, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(state_dim + action_dim, mid_dim), nn.ReLU(),\n",
    "                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),\n",
    "                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),\n",
    "                                 nn.Linear(mid_dim, 1))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat((state, action), dim=1))  # q value\n",
    "\n",
    "\n",
    "class CriticAdv(nn.Module):\n",
    "    def __init__(self, mid_dim, state_dim, _action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),\n",
    "                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),\n",
    "                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),\n",
    "                                 nn.Linear(mid_dim, 1))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)  # advantage value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, mid_dim, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(state_dim, mid_dim), nn.ReLU(),\n",
    "                                 nn.Linear(mid_dim, mid_dim), nn.ReLU(),\n",
    "                                 nn.Linear(mid_dim, mid_dim), nn.Hardswish(),\n",
    "                                 nn.Linear(mid_dim, action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state).tanh()  # action.tanh()\n",
    "\n",
    "    def get_action(self, state, action_std):\n",
    "        action = self.net(state).tanh()\n",
    "        noise = (torch.randn_like(action) * action_std).clamp(-0.5, 0.5)\n",
    "        return (action + noise).clamp(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBase:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.device = None\n",
    "        self.action_dim = None\n",
    "        self.if_off_policy = None\n",
    "        self.explore_noise = None\n",
    "        self.trajectory_list = None\n",
    "\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        self.cri = self.cri_target = self.if_use_cri_target = self.cri_optim = self.ClassCri = None\n",
    "        self.act = self.act_target = self.if_use_act_target = self.act_optim = self.ClassAct = None\n",
    "\n",
    "    def init(self, net_dim, state_dim, action_dim, learning_rate=1e-4, _if_per_or_gae=False, gpu_id=0):\n",
    "        # explict call self.init() for multiprocessing\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.cri = self.ClassCri(net_dim, state_dim, action_dim).to(self.device)\n",
    "        self.act = self.ClassAct(net_dim, state_dim, action_dim).to(self.device) if self.ClassAct else self.cri\n",
    "        self.cri_target = deepcopy(self.cri) if self.if_use_cri_target else self.cri\n",
    "        self.act_target = deepcopy(self.act) if self.if_use_act_target else self.act\n",
    "\n",
    "        self.cri_optim = torch.optim.Adam(self.cri.parameters(), learning_rate)\n",
    "        self.act_optim = torch.optim.Adam(self.act.parameters(), learning_rate) if self.ClassAct else self.cri\n",
    "        del self.ClassCri, self.ClassAct\n",
    "\n",
    "    def select_action(self, state) -> np.ndarray:\n",
    "        states = torch.as_tensor((state,), dtype=torch.float32, device=self.device)\n",
    "        action = self.act(states)[0]\n",
    "        action = (action + torch.randn_like(action) * self.explore_noise).clamp(-1, 1)\n",
    "        return action.detach().cpu().numpy()\n",
    "    def explore_env(self, env, target_step):\n",
    "        trajectory = list()\n",
    "\n",
    "        state = self.state\n",
    "        for _ in range(target_step):\n",
    "            action = self.select_action(state)\n",
    "            \n",
    "            state,next_state, reward, done, = env.step(action)\n",
    "\n",
    "            trajectory.append((state, (reward, done, *action)))\n",
    "            state = env.reset() if done else next_state\n",
    "        self.state = state\n",
    "        return trajectory\n",
    "    @staticmethod\n",
    "    def optim_update(optimizer, objective):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net, current_net, tau):\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "    def save_or_load_agent(self, cwd, if_save):\n",
    "        def load_torch_file(model_or_optim, _path):\n",
    "            state_dict = torch.load(_path, map_location=lambda storage, loc: storage)\n",
    "            model_or_optim.load_state_dict(state_dict)\n",
    "\n",
    "        name_obj_list = [('actor', self.act), ('act_target', self.act_target), ('act_optim', self.act_optim),\n",
    "                         ('critic', self.cri), ('cri_target', self.cri_target), ('cri_optim', self.cri_optim), ]\n",
    "        name_obj_list = [(name, obj) for name, obj in name_obj_list if obj is not None]\n",
    "        if if_save:\n",
    "            for name, obj in name_obj_list:\n",
    "                save_path = f\"{cwd}/{name}.pth\"\n",
    "                torch.save(obj.state_dict(), save_path)\n",
    "        else:\n",
    "            for name, obj in name_obj_list:\n",
    "                save_path = f\"{cwd}/{name}.pth\"\n",
    "                load_torch_file(obj, save_path) if os.path.isfile(save_path) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDDPG(AgentBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.explore_noise = 0.1  # explore noise of action\n",
    "        self.if_use_cri_target = self.if_use_act_target = True\n",
    "        self.ClassCri = Critic\n",
    "        self.ClassAct = Actor\n",
    "\n",
    "    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau) -> (float, float):\n",
    "        buffer.update_now_len()\n",
    "        obj_critic = obj_actor = None\n",
    "        for _ in range(int(buffer.now_len / batch_size * repeat_times)):\n",
    "            obj_critic, state = self.get_obj_critic(buffer, batch_size)#critic loss \n",
    "            self.optim_update(self.cri_optim, obj_critic)\n",
    "            self.soft_update(self.cri_target, self.cri, soft_update_tau)\n",
    "\n",
    "            action_pg = self.act(state)  # policy gradient\n",
    "            obj_actor = -self.cri(state, action_pg).mean()# actor loss, makes it bigger\n",
    "            self.optim_update(self.act_optim, obj_actor)\n",
    "            self.soft_update(self.act_target, self.act, soft_update_tau)\n",
    "        return obj_actor.item(), obj_critic.item()\n",
    "\n",
    "    def get_obj_critic(self, buffer, batch_size) -> (torch.Tensor, torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            reward, mask, action, state, next_s = buffer.sample_batch(batch_size)\n",
    "            next_q = self.cri_target(next_s, self.act_target(next_s))\n",
    "            q_label = reward + mask * next_q\n",
    "        q_value = self.cri(state, action)\n",
    "        obj_critic = self.criterion(q_value, q_label)\n",
    "        return obj_critic, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_buffer(_trajectory):\n",
    "    ten_state = torch.as_tensor([item[0] for item in _trajectory], dtype=torch.float32)\n",
    "    ary_other = torch.as_tensor([item[1] for item in _trajectory])\n",
    "    ary_other[:, 0] = ary_other[:, 0]   # ten_reward\n",
    "    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma\n",
    "\n",
    "    buffer.extend_buffer(ten_state, ary_other)\n",
    "\n",
    "    _steps = ten_state.shape[0]\n",
    "    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)\n",
    "    return _steps, _r_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define auxiliary functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_buffer(_trajectory):\n",
    "    ten_state = torch.as_tensor([item[0] for item in _trajectory], dtype=torch.float32)\n",
    "    ary_other = torch.as_tensor([item[1] for item in _trajectory])\n",
    "    ary_other[:, 0] = ary_other[:, 0]   # ten_reward\n",
    "    ary_other[:, 1] = (1.0 - ary_other[:, 1]) * gamma  # ten_mask = (1.0 - ary_done) * gamma\n",
    "\n",
    "    buffer.extend_buffer(ten_state, ary_other)\n",
    "\n",
    "    _steps = ten_state.shape[0]\n",
    "    _r_exp = ary_other[:, 0].mean()  # other = (reward, mask, action)\n",
    "    return _steps, _r_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    '''revise here for our own purpose'''\n",
    "    def __init__(self, agent=None, env=None):\n",
    "        self.agent = agent  # Deep Reinforcement Learning algorithm\n",
    "        self.env = env  # the environment for training\n",
    "        self.plot_shadow_on=False# control do we need to plot all shadow figures\n",
    "        self.cwd = None  # current work directory. None means set automatically\n",
    "        self.if_remove = False  # remove the cwd folder? (True, False, None:ask me)\n",
    "        # self.replace_train_data=True\n",
    "        self.visible_gpu = '0,1,2,3'  # for example: os.environ['CUDA_VISIBLE_DEVICES'] = '0, 2,'\n",
    "        self.worker_num = 2  # rollout workers number pre GPU (adjust it to get high GPU usage)\n",
    "        self.num_threads = 8  # cpu_num for evaluate model, torch.set_num_threads(self.num_threads)\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.num_episode=2000\n",
    "        self.gamma = 0.995  # discount factor of future rewards\n",
    "        # self.reward_scale = 1  # an approximate target reward usually be closed to 256\n",
    "        self.learning_rate = 2 ** -14  # 2 ** -14 ~= 6e-5\n",
    "        self.soft_update_tau = 2 ** -8  # 2 ** -8 ~= 5e-3\n",
    "\n",
    "        self.net_dim = 256  # the network width 256\n",
    "        self.batch_size = 4096  # num of transitions sampled from replay buffer.\n",
    "        self.repeat_times = 2 ** 5  # repeatedly update network to keep critic's loss small\n",
    "        self.target_step = 4096 # collect target_step experiences , then update network, 1024\n",
    "        self.max_memo = 500000  # capacity of replay buffer\n",
    "        self.if_per_or_gae = False  # PER for off-policy sparse reward: Prioritized Experience Replay.\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        # self.eval_gap = 2 ** 6  # evaluate the agent per eval_gap seconds\n",
    "        # self.eval_times = 2  # number of times that get episode return in first\n",
    "        self.random_seed = 0  # initialize random seed in self.init_before_training()\n",
    "        self.random_seed_list=[1234,2234,3234,4234,5234]\n",
    "        '''Arguments for save and plot issues'''\n",
    "        self.train=True\n",
    "        self.save_network=True\n",
    "        self.test_network=True\n",
    "        self.save_test_data=True\n",
    "        self.compare_with_pyomo=True\n",
    "        self.plot_on=True \n",
    "\n",
    "    def init_before_training(self, if_main):\n",
    "        if self.cwd is None:\n",
    "            agent_name = self.agent.__class__.__name__\n",
    "            self.cwd = f'./{agent_name}'\n",
    "\n",
    "        if if_main:\n",
    "            import shutil  # remove history according to bool(if_remove)\n",
    "            if self.if_remove is None:\n",
    "                self.if_remove = bool(input(f\"| PRESS 'y' to REMOVE: {self.cwd}? \") == 'y')\n",
    "            elif self.if_remove:\n",
    "                shutil.rmtree(self.cwd, ignore_errors=True)\n",
    "                print(f\"| Remove cwd: {self.cwd}\")\n",
    "            os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "        np.random.seed(self.random_seed)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        torch.set_num_threads(self.num_threads)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(self.visible_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_len, state_dim, action_dim, gpu_id=0):\n",
    "        self.now_len = 0\n",
    "        self.next_idx = 0\n",
    "        self.if_full = False\n",
    "        self.max_len = max_len\n",
    "        self.data_type = torch.float32\n",
    "        self.action_dim = action_dim\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        other_dim = 1 + 1 + self.action_dim\n",
    "        self.buf_other = torch.empty(size=(max_len, other_dim), dtype=self.data_type, device=self.device)\n",
    "\n",
    "        if isinstance(state_dim, int):  # state is pixel\n",
    "            self.buf_state = torch.empty((max_len, state_dim), dtype=torch.float32, device=self.device)\n",
    "        elif isinstance(state_dim, tuple):\n",
    "            self.buf_state = torch.empty((max_len, *state_dim), dtype=torch.uint8, device=self.device)\n",
    "        else:\n",
    "            raise ValueError('state_dim')\n",
    "\n",
    "    def extend_buffer(self, state, other):  # CPU array to CPU array\n",
    "        size = len(other)\n",
    "        next_idx = self.next_idx + size\n",
    "\n",
    "        if next_idx > self.max_len:\n",
    "            self.buf_state[self.next_idx:self.max_len] = state[:self.max_len - self.next_idx]\n",
    "            self.buf_other[self.next_idx:self.max_len] = other[:self.max_len - self.next_idx]\n",
    "            self.if_full = True\n",
    "\n",
    "            next_idx = next_idx - self.max_len\n",
    "            self.buf_state[0:next_idx] = state[-next_idx:]\n",
    "            self.buf_other[0:next_idx] = other[-next_idx:]\n",
    "        else:\n",
    "            self.buf_state[self.next_idx:next_idx] = state\n",
    "            self.buf_other[self.next_idx:next_idx] = other\n",
    "        self.next_idx = next_idx\n",
    "\n",
    "    def sample_batch(self, batch_size) -> tuple:\n",
    "        indices = rd.randint(self.now_len - 1, size=batch_size)\n",
    "        r_m_a = self.buf_other[indices]\n",
    "        return (r_m_a[:, 0:1],\n",
    "                r_m_a[:, 1:2],\n",
    "                r_m_a[:, 2:],\n",
    "                self.buf_state[indices],\n",
    "                self.buf_state[indices + 1])\n",
    "\n",
    "    def update_now_len(self):\n",
    "        self.now_len = self.max_len if self.if_full else self.next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_return(env, act, device):\n",
    "    episode_return = 0.0  # sum of rewards in an episode\n",
    "    episode_unbalance=0.0\n",
    "    state = env.reset()\n",
    "    for i in range(24):\n",
    "        s_tensor = torch.as_tensor((state,), device=device)\n",
    "        a_tensor = act(s_tensor)\n",
    "        action = a_tensor.detach().cpu().numpy()[0]  # not need detach(), because with torch.no_grad() outside\n",
    "        state, next_state, reward, done,= env.step(action)\n",
    "        state=next_state\n",
    "        episode_return += reward\n",
    "        episode_unbalance+=env.real_unbalance\n",
    "        if done:\n",
    "            break\n",
    "    return episode_return,episode_unbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxtvTMQM0knh"
   },
   "source": [
    "## Train and evaluate by DDPG algorithms \n",
    "\n",
    "[Least absolute shrinkage and selection operator (LASSO)](https://www.jstor.org/stable/2346178#metadata_info_tab_contents), is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "Lasso was originally formulated for linear regression models. This simple case reveals a substantial amount about the estimator. These include its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression) the coefficient estimates do not need to be unique if covariates are collinear.\n",
    "\n",
    "In this part, LASSO is implemented to train the price forecast model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Arguments()\n",
    "    '''here record real unbalance'''\n",
    "    reward_record={'episode':[],'steps':[],'mean_episode_reward':[],'unbalance':[]}\n",
    "    loss_record={'episode':[],'steps':[],'critic_loss':[],'actor_loss':[],'entropy_loss':[]}\n",
    "    args.visible_gpu = '2'\n",
    "    for seed in args.random_seed_list:\n",
    "        args.random_seed=seed\n",
    "        #set different seed \n",
    "        args.agent=AgentDDPG()\n",
    "        agent_name=f'{args.agent.__class__.__name__}'\n",
    "        args.agent.cri_target=True\n",
    "        args.env=ESSEnv()\n",
    "        # creat lists of lists/or creat a long list? \n",
    "        \n",
    "        args.init_before_training(if_main=True)\n",
    "        '''init agent and environment'''\n",
    "        agent=args.agent\n",
    "        env=args.env\n",
    "        agent.init(args.net_dim,env.state_space.shape[0],env.action_space.shape[0],args.learning_rate,args.if_per_or_gae)\n",
    "        '''init replay buffer'''\n",
    "        buffer = ReplayBuffer(max_len=args.max_memo, state_dim=env.state_space.shape[0],\n",
    "                            action_dim= env.action_space.shape[0])    \n",
    "        '''start training'''\n",
    "        cwd=args.cwd\n",
    "        gamma=args.gamma\n",
    "        batch_size=args.batch_size# how much data should be used to update net\n",
    "        target_step=args.target_step#how manysteps of one episode should stop\n",
    "        # reward_scale=args.reward_scale# here we use it as 1# we dont need this in our model\n",
    "        repeat_times=args.repeat_times# how many times should update for one batch size data \n",
    "        # if_allow_break = args.if_allow_break\n",
    "        soft_update_tau = args.soft_update_tau\n",
    "        # get the first experience from \n",
    "        agent.state=env.reset()\n",
    "        # trajectory=agent.explore_env(env,target_step)\n",
    "        # update_buffer(trajectory)\n",
    "        '''collect data and train and update network'''\n",
    "        num_episode=args.num_episode\n",
    "\n",
    "        ##\n",
    "        # args.train=False\n",
    "        # args.save_network=False\n",
    "        # args.test_network=False\n",
    "        # args.save_test_data=False\n",
    "        # args.compare_with_pyomo=False\n",
    "        #\n",
    "        if args.train:\n",
    "            collect_data=True\n",
    "            while collect_data:\n",
    "                print(f'buffer:{buffer.now_len}')\n",
    "                with torch.no_grad():\n",
    "                    trajectory=agent.explore_env(env,target_step)\n",
    "                    steps,r_exp=update_buffer(trajectory)\n",
    "                    buffer.update_now_len()\n",
    "                if buffer.now_len>=10000:\n",
    "                    collect_data=False\n",
    "            for i_episode in range(num_episode):\n",
    "                critic_loss,actor_loss=agent.update_net(buffer,batch_size,repeat_times,soft_update_tau)\n",
    "                loss_record['critic_loss'].append(critic_loss)\n",
    "                loss_record['actor_loss'].append(actor_loss)\n",
    "                with torch.no_grad():\n",
    "                    episode_reward,episode_unbalance=get_episode_return(env,agent.act,agent.device)\n",
    "                    reward_record['mean_episode_reward'].append(episode_reward)\n",
    "                    reward_record['unbalance'].append(episode_unbalance)\n",
    "                print(f'curren epsiode is {i_episode}, reward:{episode_reward},unbalance:{episode_unbalance},buffer_length: {buffer.now_len}')\n",
    "                if i_episode % 10==0:\n",
    "                    # target_step\n",
    "                    with torch.no_grad():\n",
    "                        trajectory=agent.explore_env(env,target_step)\n",
    "                        steps,r_exp=update_buffer(trajectory)\n",
    "    loss_record_path=f'{args.cwd}/loss_data.pkl'\n",
    "    reward_record_path=f'{args.cwd}/reward_data.pkl'\n",
    "    # current only store last seed corresponded actor \n",
    "    act_save_path=f'{args.cwd}/actor.pth'\n",
    "    # args.replace_train_data = False\n",
    "\n",
    "    with open (loss_record_path,'wb') as tf:\n",
    "        pickle.dump(loss_record,tf)\n",
    "    with open (reward_record_path,'wb') as tf:\n",
    "        pickle.dump(reward_record,tf)\n",
    "    print('training data have been saved')\n",
    "    if args.save_network:\n",
    "        torch.save(agent.act.state_dict(),act_save_path)\n",
    "        print('actor parameters have been saved')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "v5sB8p0Aym4O",
    "0Gzxf6aYy88L",
    "wGp3HMViz_9W",
    "pyCHTTL70J5i",
    "YxtvTMQM0knh",
    "Xv0pstSg0ySQ",
    "Z6-FU84Z1lor",
    "D34krPtG12pt"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fb18d9f503d463f84837eb694fb2457": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41b49a9eb142437f8fd5db3646d834e3",
       "IPY_MODEL_d6592afb1a1f4dc1884f4358cd43572a",
       "IPY_MODEL_5c15d5b01d7f40bbb3ec80217e136d9e"
      ],
      "layout": "IPY_MODEL_d3715cc25c384ae58bc5fba1627810e6"
     }
    },
    "1e86f90b581a4e99ab4f4eed1a3ccff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2fdd5a30f1524b0da2de6331446f3520",
      "placeholder": "​",
      "style": "IPY_MODEL_4ecf8faa6c7a48baa6cd663fded51c91",
      "value": " 24/24 [00:00&lt;00:00, 113.63it/s]"
     }
    },
    "1e8868239a184a60810ef2ec252c5294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fdd5a30f1524b0da2de6331446f3520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fe5ce4e7b6148b88357cf9621de1afe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30d8faf4327245b5ab6907b2dda62f3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3373b4d6fcb6485497e8c79c99386ac6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "34848c87a535432da4aa9d38a270b043": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41b49a9eb142437f8fd5db3646d834e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d14e0baf75a741d595e84095f2b66ce1",
      "placeholder": "​",
      "style": "IPY_MODEL_7950a0ac9c064f9e961ee97fe343a3e2",
      "value": "100%"
     }
    },
    "4ecf8faa6c7a48baa6cd663fded51c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5681483f3f6848738cb5bc0b50543e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cde58a79cf6743f7a26e25137a14a9f1",
       "IPY_MODEL_92b502573b484e39b9be4ddc8c7ebce4",
       "IPY_MODEL_1e86f90b581a4e99ab4f4eed1a3ccff3"
      ],
      "layout": "IPY_MODEL_9ed404cf391f43bbbe4d13b2856bf926"
     }
    },
    "5c15d5b01d7f40bbb3ec80217e136d9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30d8faf4327245b5ab6907b2dda62f3a",
      "placeholder": "​",
      "style": "IPY_MODEL_be72d070cac74a9bae285eb69b36c62b",
      "value": " 24/24 [00:01&lt;00:00, 17.08it/s]"
     }
    },
    "728437530b4840bea8d24ba43e499a53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7950a0ac9c064f9e961ee97fe343a3e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92b502573b484e39b9be4ddc8c7ebce4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9bd8bef0511433bace0b3fa9474ebe1",
      "max": 24,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3373b4d6fcb6485497e8c79c99386ac6",
      "value": 24
     }
    },
    "9ed404cf391f43bbbe4d13b2856bf926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9bd8bef0511433bace0b3fa9474ebe1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be72d070cac74a9bae285eb69b36c62b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cde58a79cf6743f7a26e25137a14a9f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2fe5ce4e7b6148b88357cf9621de1afe",
      "placeholder": "​",
      "style": "IPY_MODEL_728437530b4840bea8d24ba43e499a53",
      "value": "100%"
     }
    },
    "d14e0baf75a741d595e84095f2b66ce1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3715cc25c384ae58bc5fba1627810e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6592afb1a1f4dc1884f4358cd43572a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e8868239a184a60810ef2ec252c5294",
      "max": 24,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_34848c87a535432da4aa9d38a270b043",
      "value": 24
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}